diff --git a/chainerrl/action_value.py b/chainerrl/action_value.py
index 2e6a6b28..9a664382 100644
--- a/chainerrl/action_value.py
+++ b/chainerrl/action_value.py
@@ -58,12 +58,14 @@ class DiscreteActionValue(ActionValue):
 
     @cached_property
     def greedy_actions(self):
+#        print("chainer.Variable(self.q_values.array.argmax(axis=1).astype(np.int32))")
         return chainer.Variable(
             self.q_values.array.argmax(axis=1).astype(np.int32))
 
     @cached_property
     def max(self):
         with chainer.force_backprop_mode():
+#            print("F.select_item(self.q_values, self.greedy_actions)")
             return F.select_item(self.q_values, self.greedy_actions)
 
     def evaluate_actions(self, actions):
@@ -80,6 +82,7 @@ class DiscreteActionValue(ActionValue):
         return F.sum(F.softmax(beta * self.q_values) * self.q_values, axis=1)
 
     def __repr__(self):
+        print("'DiscreteActionValue greedy_actions:{} q_values:{}'.format(self.greedy_actions.array,self.q_values_formatter(self.q_values.array))")
         return 'DiscreteActionValue greedy_actions:{} q_values:{}'.format(
             self.greedy_actions.array,
             self.q_values_formatter(self.q_values.array))
@@ -89,6 +92,7 @@ class DiscreteActionValue(ActionValue):
         return (self.q_values,)
 
     def __getitem__(self, i):
+        print("DiscreteActionValue(self.q_values[i], q_values_formatter=self.q_values_formatter)")
         return DiscreteActionValue(
             self.q_values[i], q_values_formatter=self.q_values_formatter)
 
diff --git a/chainerrl/agent.py b/chainerrl/agent.py
index 9f96915d..154c5cd2 100644
--- a/chainerrl/agent.py
+++ b/chainerrl/agent.py
@@ -2,7 +2,11 @@ from abc import ABCMeta
 from abc import abstractmethod
 from abc import abstractproperty
 import os
+import sys
+import onnx_chainer
+from chainer import functions as F
 
+import chainer
 from chainer import serializers
 import numpy
 import warnings
@@ -105,8 +109,12 @@ class AttributeSavingMixin(object):
         os.makedirs(dirname, exist_ok=True)
         ancestors.append(self)
         for attr in self.saved_attributes:
+            print("Starting iteration and Attr is: ")
+            print(attr)
             assert hasattr(self, attr)
             attr_value = getattr(self, attr)
+            print("attr_valu is: ")
+            print(attr_value)
             if attr_value is None:
                 continue
             if isinstance(attr_value, AttributeSavingMixin):
@@ -114,11 +122,40 @@ class AttributeSavingMixin(object):
                     attr_value is ancestor
                     for ancestor in ancestors
                 ), "Avoid an infinite loop"
+                print("saving outside of else, and attr_value is...")
+                print(attr_value)
                 attr_value.__save(os.path.join(dirname, attr), ancestors)
             else:
-                serializers.save_npz(
-                    os.path.join(dirname, '{}.npz'.format(attr)),
-                    getattr(self, attr))
+                print("In the ELSE and attr is: ")
+                print(attr)
+                save_path = os.path.join(dirname, '{}.npz'.format(attr))
+                serializers.save_npz(save_path, getattr(self, attr))
+                save_path = os.path.join(dirname, '{}.h5'.format(attr))
+                serializers.save_hdf5(save_path, getattr(self, attr))
+#                if (attr == "model"):
+#                    numpy.set_printoptions(threshold=sys.maxsize)
+#                    print("save_path is: ")
+#                    print(save_path)
+#                    print("Weights are: ")
+#                    weights = numpy.load(save_path)
+#                    for item in weights:
+#                        print("\n Item" + item + ">>>>>>>>>>>>>>>>>>>>>>>>>")
+#                        sz = str(weights[item].size)
+#                        print("Size: " + sz)
+#                        dim = str(weights[item].ndim)
+#                        print("Dimensions: " + dim)
+#                        print(weights[item])
+#                        print(numpy.array(weights[item]))
+                    # saving as universal network model...
+                    # Prepare dummy data, not sure what the 4th value should be?
+                model = getattr(self, attr)
+                x = numpy.zeros((4, 84, 84), dtype=numpy.float32)[None]
+                # Put Chainer into inference mode
+                with chainer.using_config('train', False):
+                    #chainer_out = model(x).array
+                    chainer_out = model(x).q_values
+                # Now save model
+                onnx_model = onnx_chainer.export(getattr(self, attr), x, filename='convnet.onnx')
         ancestors.pop()
 
     def load(self, dirname):
diff --git a/chainerrl/experiments/evaluator.py b/chainerrl/experiments/evaluator.py
index c333fbc2..8dd6a9ed 100644
--- a/chainerrl/experiments/evaluator.py
+++ b/chainerrl/experiments/evaluator.py
@@ -6,7 +6,9 @@ import time
 
 import numpy as np
 
+from chainer import serializers
 import chainerrl
+import keras
 
 
 """Columns that describe information about an experiment.
@@ -256,14 +258,16 @@ def eval_performance(env, agent, n_steps, n_episodes, max_episode_len=None,
 
 
 def record_stats(outdir, values):
+    print("using custom scores recorder")
     with open(os.path.join(outdir, 'scores.txt'), 'a+') as f:
         print('\t'.join(str(x) for x in values), file=f)
 
 
 def save_agent(agent, t, outdir, logger, suffix=''):
+    print("Using custom agent save")
     dirname = os.path.join(outdir, '{}{}'.format(t, suffix))
-    agent.save(dirname)
-    logger.info('Saved the agent to %s', dirname)
+    agent.save('model')
+    logger.info('Saved the agent with hdf5 format to %s', dirname)
 
 
 class Evaluator(object):
diff --git a/chainerrl/experiments/train_agent.py b/chainerrl/experiments/train_agent.py
index 29b3b031..ffe289b9 100644
--- a/chainerrl/experiments/train_agent.py
+++ b/chainerrl/experiments/train_agent.py
@@ -9,7 +9,7 @@ from chainerrl.misc.ask_yes_no import ask_yes_no
 def save_agent_replay_buffer(agent, t, outdir, suffix='', logger=None):
     logger = logger or logging.getLogger(__name__)
     filename = os.path.join(outdir, '{}{}.replay.pkl'.format(t, suffix))
-    agent.replay_buffer.save(filename)
+    agent.replay_buffer.save_hdf5(filename)
     logger.info('Saved the current replay buffer to %s', filename)
 
 
@@ -127,7 +127,7 @@ def train_agent_with_evaluation(agent,
             the best-so-far score, the current agent is saved.
         logger (logging.Logger): Logger used in this function.
     """
-
+    print("In custom train agent with eval function")
     logger = logger or logging.getLogger(__name__)
 
     os.makedirs(outdir, exist_ok=True)
diff --git a/chainerrl/links/dqn_head.py b/chainerrl/links/dqn_head.py
index 78d48f42..1922749d 100644
--- a/chainerrl/links/dqn_head.py
+++ b/chainerrl/links/dqn_head.py
@@ -13,11 +13,16 @@ class NatureDQNHead(chainer.ChainList):
         self.n_output_channels = n_output_channels
 
         layers = [
+            #L.Convolution2D(n_input_channels, out_channel=32, ksize=8, stride=4, pad=0, nobias=False, initialW=None, initial_bias=bias, *, dilate=1, groups=1),
             L.Convolution2D(n_input_channels, 32, 8, stride=4,
                             initial_bias=bias),
+            #L.Convolution2D(n_input_channels=32, out_channel=64, ksize=4, stride=2, pad=0, nobias=False, initialW=None, initial_bias=bias, *, dilate=1, groups=1),
             L.Convolution2D(32, 64, 4, stride=2, initial_bias=bias),
+            #L.Convolution2D(n_input_channels=64, out_channel=64, ksize=3, stride=1, pad=0, nobias=False, initialW=None, initial_bias=bias, *, dilate=1, groups=1),
             L.Convolution2D(64, 64, 3, stride=1, initial_bias=bias),
+            #L.Convolution2D(in_size=3136, out_size=n_output_channels, nobias=False, initialW=None, initial_bias=bias),
             L.Linear(3136, n_output_channels, initial_bias=bias),
+#            L.Linear(7744, n_output_channels, initial_bias=bias),
         ]
 
         super(NatureDQNHead, self).__init__(*layers)
diff --git a/examples/atari/train_dqn_ale.py b/examples/atari/train_dqn_ale.py
index ea6f26c8..2bb95a9a 100644
--- a/examples/atari/train_dqn_ale.py
+++ b/examples/atari/train_dqn_ale.py
@@ -68,6 +68,7 @@ def parse_agent(agent):
 
 
 def main():
+    print("Using the customized DQN trainer")
     parser = argparse.ArgumentParser()
     parser.add_argument('--env', type=str, default='BreakoutNoFrameskip-v4',
                         help='OpenAI Atari domain to perform algorithm on.')
@@ -142,7 +143,7 @@ def main():
     test_seed = 2 ** 31 - 1 - args.seed
 
     args.outdir = experiments.prepare_output_dir(args, args.outdir)
-    print('Output files are saved in {}'.format(args.outdir))
+    print('In custom DQN file and Output files are saved in {}'.format(args.outdir))
 
     def make_env(test):
         # Use different random seeds for train and test envs
@@ -227,6 +228,7 @@ def main():
             args.eval_n_runs, eval_stats['mean'], eval_stats['median'],
             eval_stats['stdev']))
     else:
+        print("From custom DQN, calling train agent with eval")
         experiments.train_agent_with_evaluation(
             agent=agent, env=env, steps=args.steps,
             eval_n_steps=None,
